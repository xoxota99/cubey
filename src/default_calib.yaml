# Sample camera calibration file.
# We want a model that allows the camera to detect what color it's looking at for each face. 
# Our ability to accurately detect those colors is very susceptible to lighting conditions, 
# so for each environment you set up the robot, you need to take a solved cube (with a known state), 
# put it in the robot, and have the robot "look" at it to rediscover what each color looks like, 
# on each of the facelets that it will be sampling throughout the solve.

# NOTE: we *still* need half-decent lighting. In low-light conditions, blue, red, yellow, all 
# look pretty much alike, and the solver will misread those colors even with calibration.

# When sampling (or scanning) a coordinate in the captured image, average the color from a square 
# of twice this width / height in pixels.
sample_size: 10 # 20x20 pixel square

camera: 
  hue: 0.5
  saturation: 0.390625
  brightness: 0.5
  gain: 0.0
  contrast: 0.3263157894736842
  auto_exposure: 0.75   # 0.25 for manual, 0.75 for auto.
  exposure: default     # for ELP cameras, you must set auto_exposure to 0.25 in order to use manual exposure (see https://github.com/opencv/opencv/issues/9738). This also makes video capture much slower.
  
colors: {
    # specify min-max HSV brackets for each color.
    "U": { # Up (Yellow)
      min: [25,190,20],
      max: [46,255,255]
    },
    "R": { # Right (Blue)
      min: [140,109,20],
      max: [185,255,255]
    },
    "F":  { # Front (Orange)
      min: [8,170,20],
      max: [24,255,255]
    },
    "D": { # Down (White)
      min: [0,0,100],
      max: [255,60,255]
    },
    "L": { # Left (Green)
      min: [60,109,50],
      max: [100,255,255]
    },
    "B": { # Back (Red)
      min: [248,109,64],
      max: [7,255,255]
    }
}

